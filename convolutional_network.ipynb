{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('fashion_train.npy')\n",
    "test = np.load('fashion_test.npy')\n",
    "\n",
    "X_train, y_train = train[:, :784], train[:, 784]\n",
    "X_test, y_test = test[:, :784], test[:, 784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the X_train and X_test to 28x28\n",
    "X_train = X_train.reshape(-1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding =1)\n",
    "        # linear layers\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 5) \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # max pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flattening the image\n",
    "        x = x.view(-1, 7*7*16)\n",
    "        # linear layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z8/wd7qc7cs1wd8pr3h4y_6lxxw0000gn/T/ipykernel_47290/1861451064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#X_test = torch.from_numpy(X_test).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#X_train = X_train.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z8/wd7qc7cs1wd8pr3h4y_6lxxw0000gn/T/ipykernel_47290/3196214172.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# flattening the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "#create a tensor from the X_train and X_test numpy arrays\n",
    "#X_train = torch.from_numpy(X_train).float()\n",
    "#X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "net.forward(X_train)\n",
    "    \n",
    "#X_train = X_train.unsqueeze(1)\n",
    "#flatten the X_train to 1D\n",
    "X_train = torch.flatten(X_train, 1)\n",
    "\n",
    "#X_test = X_test.unsqueeze(1)\n",
    "X_test = torch.flatten(X_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGD:\n",
    "    def __init__(self, X, y, batch_size=32):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        ''' Sample a batch of data '''\n",
    "        idx = np.random.choice(self.X.shape[0], self.batch_size, replace=False)\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self, name, lr=0.01):\n",
    "        self.name = name\n",
    "        self.lr = lr\n",
    "\n",
    "    def calculate(self, x, derivative=False):\n",
    "        if self.name == 'sigmoid':\n",
    "            return self.sigmoid(x, derivative=derivative)\n",
    "        elif self.name == 'relu':\n",
    "            return self.relu(x, derivative=derivative)\n",
    "        elif self.name == 'softmax':\n",
    "            return self.softmax(x, derivative=derivative)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        ''' \n",
    "            sigmoid activation function and its derivative \n",
    "        '''\n",
    "        if not derivative:\n",
    "            return 1 / (1 + np.exp (-x))\n",
    "        else:\n",
    "            out = self.sigmoid(x)\n",
    "            return out * (1 - out)\n",
    "\n",
    "    def relu(self, x, derivative=False):\n",
    "        ''' \n",
    "            relu activation function and its derivative \n",
    "        '''\n",
    "        if not derivative:\n",
    "            return np.where(x > 0, x, 0)\n",
    "        else:\n",
    "            return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, x, derivative=False):\n",
    "        ''' \n",
    "            softmax activation function and its derivative \n",
    "        '''\n",
    "        if not derivative:\n",
    "            exps = np.exp(x - np.max(x))\n",
    "            return exps / np.sum(exps)\n",
    "        else:\n",
    "            out = self.softmax(x)\n",
    "            return out * (1 - out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size=784, hidden_size=300, output_size=5, layers_num=3, learning_rate=0.01, test=False, activation_name='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.layers_num = layers_num\n",
    "        self.lr = learning_rate\n",
    "        self.activation = ActivationFunction(activation_name, lr=learning_rate)\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.weights.append(np.random.randn(self.input_size, self.hidden_size))\n",
    "        self.bias.append(np.random.randn(1, self.hidden_size))\n",
    "\n",
    "        self.weights.append(np.random.randn(self.hidden_size, self.output_size))\n",
    "        self.bias.append(np.random.randn(1, self.output_size))\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        ''' \n",
    "            conduct the forward pass on the network \n",
    "        '''\n",
    "        #X = X / 255\n",
    "        self.z1 = np.dot(X, self.weights[0]) + self.bias[0]\n",
    "        self.a1 = self.activation.calculate(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights[1]) + self.bias[1]\n",
    "        self.a2 = self.activation.calculate(self.z2)\n",
    "\n",
    "        self.outputs = np.zeros((len(self.a2), self.output_size))\n",
    "        for i in range(len(self.a2)):\n",
    "            self.outputs[i][np.argmax(self.a2[i])] = 1\n",
    "\n",
    "        self.outputs = np.array(self.outputs)\n",
    "\n",
    "\n",
    "    def backward_pass(self, X, y):\n",
    "        '''\n",
    "            conduct the backward pass on the network\n",
    "        '''\n",
    "        #X = X / 255\n",
    "        y_mtrix = np.zeros((len(y), int(self.output_size))) \n",
    "        #change y into 1-hot encoding by assigning 1 to the index of the label\n",
    "        for i in range(len(y)):\n",
    "            y_mtrix[i][y[i]] = 1\n",
    "\n",
    "        #loss, used to check the accuracy of the network\n",
    "        self.loss = np.sum((self.outputs - y_mtrix)**2) / (2*y_mtrix.size)\n",
    "\n",
    "        #accuracy, used to check the accuracy of the network\n",
    "        self.accuracy = np.sum(np.argmax(self.outputs, axis=1) == y) / len(y)\n",
    "\n",
    "        #calculate the error of the hidden layer\n",
    "        self.e1 = self.a2 - y_mtrix\n",
    "        dw1 = self.e1 * self.activation.calculate(self.a2, True)\n",
    "        \n",
    "        #calculate the error of the input layer\n",
    "        self.e2 = np.dot(dw1, self.weights[1].T)\n",
    "        dw2 = self.e2 * self.activation.calculate(self.a1, True)\n",
    "\n",
    "        #update the weights\n",
    "        w2_update = np.dot(self.a1.T, dw1) / len(X)\n",
    "        w1_update = np.dot(X.T, dw2) / len(X)\n",
    "\n",
    "        #update the biases\n",
    "        b2_update = self.lr * np.sum(dw1, axis=0, keepdims=True) / len(X)\n",
    "        b1_update = self.lr * np.sum(dw2, axis=0, keepdims=True) / len(X) \n",
    "\n",
    "        self.weights[1] -= self.lr * w2_update\n",
    "        self.weights[0] -= self.lr * w1_update\n",
    "\n",
    "        self.bias[1] -= self.lr * b2_update\n",
    "        self.bias[0] -= self.lr * b1_update\n",
    "\n",
    "        \n",
    "    def TRAIN(self, X, y, epochs=5, testing=False):\n",
    "        '''\n",
    "            train the network for a given number of epochs\n",
    "        '''\n",
    "        for epoch in range(epochs):\n",
    "            X_sample, y_sample = MiniBatchGD(X, y, batch_size=64).sample()\n",
    "            self.forward_pass(X_sample)\n",
    "            self.backward_pass(X_sample, y_sample)\n",
    "            if testing: print(f'Epoch {epoch}, loss: {self.loss}, accuracy: {self.accuracy}')\n",
    "\n",
    "    def TEST(self, X, y):\n",
    "        '''\n",
    "            test the network\n",
    "        '''\n",
    "        self.forward_pass(X)\n",
    "        self.backward_pass(X, y)\n",
    "        print(f'loss: {self.loss}, accuracy: {self.accuracy}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z8/wd7qc7cs1wd8pr3h4y_6lxxw0000gn/T/ipykernel_47290/477483474.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp (-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.175, accuracy: 0.125\n",
      "Epoch 1, loss: 0.15, accuracy: 0.25\n",
      "Epoch 2, loss: 0.14375, accuracy: 0.28125\n",
      "Epoch 3, loss: 0.134375, accuracy: 0.328125\n",
      "Epoch 4, loss: 0.175, accuracy: 0.125\n",
      "Epoch 5, loss: 0.115625, accuracy: 0.421875\n",
      "Epoch 6, loss: 0.121875, accuracy: 0.390625\n",
      "Epoch 7, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 8, loss: 0.09375, accuracy: 0.53125\n",
      "Epoch 9, loss: 0.0875, accuracy: 0.5625\n",
      "Epoch 10, loss: 0.121875, accuracy: 0.390625\n",
      "Epoch 11, loss: 0.125, accuracy: 0.375\n",
      "Epoch 12, loss: 0.096875, accuracy: 0.515625\n",
      "Epoch 13, loss: 0.08125, accuracy: 0.59375\n",
      "Epoch 14, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 15, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 16, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 17, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 18, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 19, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 20, loss: 0.109375, accuracy: 0.453125\n",
      "Epoch 21, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 22, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 23, loss: 0.075, accuracy: 0.625\n",
      "Epoch 24, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 25, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 26, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 27, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 28, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 29, loss: 0.075, accuracy: 0.625\n",
      "Epoch 30, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 31, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 32, loss: 0.05, accuracy: 0.75\n",
      "Epoch 33, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 34, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 35, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 36, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 37, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 38, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 39, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 40, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 41, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 42, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 43, loss: 0.05, accuracy: 0.75\n",
      "Epoch 44, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 45, loss: 0.090625, accuracy: 0.546875\n",
      "Epoch 46, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 47, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 48, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 49, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 50, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 51, loss: 0.0875, accuracy: 0.5625\n",
      "Epoch 52, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 53, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 54, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 55, loss: 0.05, accuracy: 0.75\n",
      "Epoch 56, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 57, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 58, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 59, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 60, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 61, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 62, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 63, loss: 0.075, accuracy: 0.625\n",
      "Epoch 64, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 65, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 66, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 67, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 68, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 69, loss: 0.05, accuracy: 0.75\n",
      "Epoch 70, loss: 0.05, accuracy: 0.75\n",
      "Epoch 71, loss: 0.05, accuracy: 0.75\n",
      "Epoch 72, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 73, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 74, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 75, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 76, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 77, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 78, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 79, loss: 0.08125, accuracy: 0.59375\n",
      "Epoch 80, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 81, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 82, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 83, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 84, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 85, loss: 0.075, accuracy: 0.625\n",
      "Epoch 86, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 87, loss: 0.05, accuracy: 0.75\n",
      "Epoch 88, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 89, loss: 0.05, accuracy: 0.75\n",
      "Epoch 90, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 91, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 92, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 93, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 94, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 95, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 96, loss: 0.08125, accuracy: 0.59375\n",
      "Epoch 97, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 98, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 99, loss: 0.06875, accuracy: 0.65625\n",
      "loss: 0.06132, accuracy: 0.6934\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(activation_name='sigmoid')\n",
    "nn.TRAIN(X_train, y_train, epochs=100, testing=True)\n",
    "nn.TEST(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef166213a119dea7a0b1a64e774e94a94a893f8530fa4c669d6e23575311d886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

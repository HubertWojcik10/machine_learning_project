{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Shirt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('fashion_train.npy')\n",
    "test = np.load('fashion_test.npy')\n",
    "\n",
    "X_train, y_train = train[:, :784], train[:, 784]\n",
    "X_test, y_test = test[:, :784], test[:, 784]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data: (10000, 784)\n",
      "Shape of the testing data: (5000, 784)\n",
      "\n",
      "Mean of the training data: 77.02673711734694\n",
      "Standard Deviation of the training data: 89.9969501937854\n",
      "\n",
      "Min of the training data: 0\n",
      "Max of the training data: 255\n"
     ]
    }
   ],
   "source": [
    "#shape of the data\n",
    "print(f'Shape of the training data: {X_train.shape}')\n",
    "print(f'Shape of the testing data: {X_test.shape}')\n",
    "\n",
    "#mean, std, min, max\n",
    "print(f'\\nMean of the training data: {X_train.mean()}')\n",
    "print(f'Standard Deviation of the training data: {X_train.std()}')\n",
    "\n",
    "print(f'\\nMin of the training data: {X_train.min()}')\n",
    "print(f'Max of the training data: {X_train.max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the distribution of the target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_distribution(y):\n",
    "    ''' Plot the distribution of the labels '''\n",
    "    classes, classes_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(x=classes, y=classes_counts, ax=ax);\n",
    "    ax.set_title('Class distribution', fontsize=16, fontweight='bold');\n",
    "    ax.set_ylabel('Count');\n",
    "    ax.set_xticklabels(NAMES);\n",
    "\n",
    "\n",
    "#plot_y_distribution(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the properties of images in the dataset?\n",
    "We investigate mean, median, and stdev of the pixel values in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(X, y, measure='mean'):\n",
    "    ''' Plot the mean, median, or std of the images'''\n",
    "    classes = np.unique(y)\n",
    "    fig, ax = plt.subplots(1, len(classes), figsize=(20, 8))\n",
    "    for c in classes:\n",
    "        imgs = X[y == c]\n",
    "\n",
    "        if measure == 'mean': av_img = np.mean(imgs, axis=0).reshape(28, 28)\n",
    "        elif measure == 'median': av_img = np.median(imgs, axis=0).reshape(28, 28)\n",
    "        elif measure == 'std': av_img = np.std(imgs, axis=0).reshape(28, 28)\n",
    "\n",
    "        ax[c].imshow(av_img, cmap='gray')\n",
    "        ax[c].set_title(NAMES[c], fontsize=16, fontweight='bold')\n",
    "\n",
    "\n",
    "#plot_images(X_train, y_train)\n",
    "#plot_images(X_train, y_train, measure='std')\n",
    "#plot_images(X_train, y_train, measure='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also investigate the distribution of the pixel values in the images. As an input, we use the mean of the pixel values in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_distribution(X, y):\n",
    "    ''' Plot the distribution of the colors '''\n",
    "    classes = np.unique(y)\n",
    "    av_imgs = np.zeros((len(classes), 28, 28))\n",
    "    for c in classes:\n",
    "        imgs = X[y == c]\n",
    "        av_imgs[c] = np.mean(imgs, axis=0).reshape(28, 28)\n",
    "\n",
    "    #plot the distribution of the average image\n",
    "    fig, ax = plt.subplots(1, len(classes), figsize=(30, 6))\n",
    "    for c in classes:\n",
    "        sns.histplot(av_imgs[c].flatten(), ax=ax[c], kde=True)\n",
    "        ax[c].set_title(NAMES[c], fontsize=16, fontweight='bold')\n",
    "        ax[c].set_xlabel('Pixel value')\n",
    "        ax[c].set_ylabel('Count')\n",
    "\n",
    "#plot_color_distribution(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGD:\n",
    "    def __init__(self, X, y, batch_size=32):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        ''' Sample a batch of data '''\n",
    "        idx = np.random.choice(self.X.shape[0], self.batch_size, replace=False)\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self, name, lr=0.01):\n",
    "        self.name = name\n",
    "        self.lr = lr\n",
    "\n",
    "    def calculate(self, x, derivative=False):\n",
    "        if self.name == 'sigmoid':\n",
    "            return self.sigmoid(x, derivative=derivative)\n",
    "        elif self.name == 'relu':\n",
    "            return self.relu(x, derivative=derivative)\n",
    "        elif self.name == 'softmax':\n",
    "            return self.softmax(x, derivative=derivative)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        ''' \n",
    "            sigmoid activation function and its derivative \n",
    "        '''\n",
    "        if not derivative:\n",
    "            return 1 / (1 + np.exp (-x))\n",
    "        else:\n",
    "            out = self.sigmoid(x)\n",
    "            return out * (1 - out)\n",
    "\n",
    "    def relu(self, x, derivative=False):\n",
    "        ''' \n",
    "            relu activation function and its derivative \n",
    "        '''\n",
    "        if not derivative:\n",
    "            return np.where(x > 0, x, 0)\n",
    "        else:\n",
    "            return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, x, derivative=False):\n",
    "        ''' \n",
    "            softmax activation function and its derivative \n",
    "        '''\n",
    "        if not derivative:\n",
    "            exps = np.exp(x - np.max(x))\n",
    "            return exps / np.sum(exps)\n",
    "        else:\n",
    "            out = self.softmax(x)\n",
    "            return out * (1 - out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size=784, hidden_size=300, output_size=5, layers_num=3, learning_rate=0.01, test=False, activation_name='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.layers_num = layers_num\n",
    "        self.lr = learning_rate\n",
    "        self.activation = ActivationFunction(activation_name, lr=learning_rate)\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.weights.append(np.random.randn(self.input_size, self.hidden_size))\n",
    "        self.bias.append(np.random.randn(1, self.hidden_size))\n",
    "\n",
    "        self.weights.append(np.random.randn(self.hidden_size, self.output_size))\n",
    "        self.bias.append(np.random.randn(1, self.output_size))\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        ''' \n",
    "            conduct the forward pass on the network \n",
    "        '''\n",
    "        #X = X / 255\n",
    "        self.z1 = np.dot(X, self.weights[0]) + self.bias[0]\n",
    "        self.a1 = self.activation.calculate(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights[1]) + self.bias[1]\n",
    "        self.a2 = self.activation.calculate(self.z2)\n",
    "\n",
    "        self.outputs = np.zeros((len(self.a2), self.output_size))\n",
    "        for i in range(len(self.a2)):\n",
    "            self.outputs[i][np.argmax(self.a2[i])] = 1\n",
    "\n",
    "        self.outputs = np.array(self.outputs)\n",
    "\n",
    "\n",
    "    def backward_pass(self, X, y):\n",
    "        '''\n",
    "            conduct the backward pass on the network\n",
    "        '''\n",
    "        #X = X / 255\n",
    "        y_mtrix = np.zeros((len(y), int(self.output_size))) \n",
    "        #change y into 1-hot encoding by assigning 1 to the index of the label\n",
    "        for i in range(len(y)):\n",
    "            y_mtrix[i][y[i]] = 1\n",
    "\n",
    "        #loss, used to check the accuracy of the network\n",
    "        self.loss = np.sum((self.outputs - y_mtrix)**2) / (2*y_mtrix.size)\n",
    "\n",
    "        #accuracy, used to check the accuracy of the network\n",
    "        self.accuracy = np.sum(np.argmax(self.outputs, axis=1) == y) / len(y)\n",
    "\n",
    "        #calculate the error of the hidden layer\n",
    "        self.e1 = self.a2 - y_mtrix\n",
    "        dw1 = self.e1 * self.activation.calculate(self.a2, True)\n",
    "        \n",
    "        #calculate the error of the input layer\n",
    "        self.e2 = np.dot(dw1, self.weights[1].T)\n",
    "        dw2 = self.e2 * self.activation.calculate(self.a1, True)\n",
    "\n",
    "        #update the weights\n",
    "        w2_update = np.dot(self.a1.T, dw1) / len(X)\n",
    "        w1_update = np.dot(X.T, dw2) / len(X)\n",
    "\n",
    "        #update the biases\n",
    "        b2_update = self.lr * np.sum(dw1, axis=0, keepdims=True) / len(X)\n",
    "        b1_update = self.lr * np.sum(dw2, axis=0, keepdims=True) / len(X) \n",
    "\n",
    "        self.weights[1] -= self.lr * w2_update\n",
    "        self.weights[0] -= self.lr * w1_update\n",
    "\n",
    "        self.bias[1] -= self.lr * b2_update\n",
    "        self.bias[0] -= self.lr * b1_update\n",
    "\n",
    "        \n",
    "    def TRAIN(self, X, y, epochs=5, testing=False):\n",
    "        '''\n",
    "            train the network for a given number of epochs\n",
    "        '''\n",
    "        for epoch in range(epochs):\n",
    "            X_sample, y_sample = MiniBatchGD(X, y, batch_size=64).sample()\n",
    "            self.forward_pass(X_sample)\n",
    "            self.backward_pass(X_sample, y_sample)\n",
    "            if testing: print(f'Epoch {epoch}, loss: {self.loss}, accuracy: {self.accuracy}')\n",
    "\n",
    "    def TEST(self, X, y):\n",
    "        '''\n",
    "            test the network\n",
    "        '''\n",
    "        self.forward_pass(X)\n",
    "        self.backward_pass(X, y)\n",
    "        print(f'loss: {self.loss}, accuracy: {self.accuracy}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.153125, accuracy: 0.234375\n",
      "Epoch 1, loss: 0.125, accuracy: 0.375\n",
      "Epoch 2, loss: 0.15625, accuracy: 0.21875\n",
      "Epoch 3, loss: 0.1625, accuracy: 0.1875\n",
      "Epoch 4, loss: 0.121875, accuracy: 0.390625\n",
      "Epoch 5, loss: 0.125, accuracy: 0.375\n",
      "Epoch 6, loss: 0.1125, accuracy: 0.4375\n",
      "Epoch 7, loss: 0.1375, accuracy: 0.3125\n",
      "Epoch 8, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 9, loss: 0.10625, accuracy: 0.46875\n",
      "Epoch 10, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 11, loss: 0.109375, accuracy: 0.453125\n",
      "Epoch 12, loss: 0.08125, accuracy: 0.59375\n",
      "Epoch 13, loss: 0.103125, accuracy: 0.484375\n",
      "Epoch 14, loss: 0.08125, accuracy: 0.59375\n",
      "Epoch 15, loss: 0.090625, accuracy: 0.546875\n",
      "Epoch 16, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 17, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 18, loss: 0.1, accuracy: 0.5\n",
      "Epoch 19, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 20, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 21, loss: 0.1, accuracy: 0.5\n",
      "Epoch 22, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 23, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 24, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 25, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 26, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 27, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 28, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 29, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 30, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 31, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 32, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 33, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 34, loss: 0.05, accuracy: 0.75\n",
      "Epoch 35, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 36, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 37, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 38, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 39, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 40, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 41, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 42, loss: 0.05, accuracy: 0.75\n",
      "Epoch 43, loss: 0.05, accuracy: 0.75\n",
      "Epoch 44, loss: 0.075, accuracy: 0.625\n",
      "Epoch 45, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 46, loss: 0.1, accuracy: 0.5\n",
      "Epoch 47, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 48, loss: 0.05, accuracy: 0.75\n",
      "Epoch 49, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 50, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 51, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 52, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 53, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 54, loss: 0.10625, accuracy: 0.46875\n",
      "Epoch 55, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 56, loss: 0.05, accuracy: 0.75\n",
      "Epoch 57, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 58, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 59, loss: 0.05, accuracy: 0.75\n",
      "Epoch 60, loss: 0.05, accuracy: 0.75\n",
      "Epoch 61, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 62, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 63, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 64, loss: 0.05, accuracy: 0.75\n",
      "Epoch 65, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 66, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 67, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 68, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 69, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 70, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 71, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 72, loss: 0.05, accuracy: 0.75\n",
      "Epoch 73, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 74, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 75, loss: 0.075, accuracy: 0.625\n",
      "Epoch 76, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 77, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 78, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 79, loss: 0.05, accuracy: 0.75\n",
      "Epoch 80, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 81, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 82, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 83, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 84, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 85, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 86, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 87, loss: 0.05, accuracy: 0.75\n",
      "Epoch 88, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 89, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 90, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 91, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 92, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 93, loss: 0.05, accuracy: 0.75\n",
      "Epoch 94, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 95, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 96, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 97, loss: 0.05, accuracy: 0.75\n",
      "Epoch 98, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 99, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 100, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 101, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 102, loss: 0.05, accuracy: 0.75\n",
      "Epoch 103, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 104, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 105, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 106, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 107, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 108, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 109, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 110, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 111, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 112, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 113, loss: 0.025, accuracy: 0.875\n",
      "Epoch 114, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 115, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 116, loss: 0.096875, accuracy: 0.515625\n",
      "Epoch 117, loss: 0.05, accuracy: 0.75\n",
      "Epoch 118, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 119, loss: 0.05, accuracy: 0.75\n",
      "Epoch 120, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 121, loss: 0.05, accuracy: 0.75\n",
      "Epoch 122, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 123, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 124, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 125, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 126, loss: 0.078125, accuracy: 0.609375\n",
      "Epoch 127, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 128, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 129, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 130, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 131, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 132, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 133, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 134, loss: 0.084375, accuracy: 0.578125\n",
      "Epoch 135, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 136, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 137, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 138, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 139, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 140, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 141, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 142, loss: 0.025, accuracy: 0.875\n",
      "Epoch 143, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 144, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 145, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 146, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 147, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 148, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 149, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 150, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 151, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 152, loss: 0.01875, accuracy: 0.90625\n",
      "Epoch 153, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 154, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 155, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 156, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 157, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 158, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 159, loss: 0.05, accuracy: 0.75\n",
      "Epoch 160, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 161, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 162, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 163, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 164, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 165, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 166, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 167, loss: 0.05, accuracy: 0.75\n",
      "Epoch 168, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 169, loss: 0.075, accuracy: 0.625\n",
      "Epoch 170, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 171, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 172, loss: 0.021875, accuracy: 0.890625\n",
      "Epoch 173, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 174, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 175, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 176, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 177, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 178, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 179, loss: 0.05, accuracy: 0.75\n",
      "Epoch 180, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 181, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 182, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 183, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 184, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 185, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 186, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 187, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 188, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 189, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 190, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 191, loss: 0.08125, accuracy: 0.59375\n",
      "Epoch 192, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 193, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 194, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 195, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 196, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 197, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 198, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 199, loss: 0.05, accuracy: 0.75\n",
      "Epoch 200, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 201, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 202, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 203, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 204, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 205, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 206, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 207, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 208, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 209, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 210, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 211, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 212, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 213, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 214, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 215, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 216, loss: 0.05, accuracy: 0.75\n",
      "Epoch 217, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 218, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 219, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 220, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 221, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 222, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 223, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 224, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 225, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 226, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 227, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 228, loss: 0.05, accuracy: 0.75\n",
      "Epoch 229, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 230, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 231, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 232, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 233, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 234, loss: 0.05, accuracy: 0.75\n",
      "Epoch 235, loss: 0.05, accuracy: 0.75\n",
      "Epoch 236, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 237, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 238, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 239, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 240, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 241, loss: 0.05, accuracy: 0.75\n",
      "Epoch 242, loss: 0.021875, accuracy: 0.890625\n",
      "Epoch 243, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 244, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 245, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 246, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 247, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 248, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 249, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 250, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 251, loss: 0.05, accuracy: 0.75\n",
      "Epoch 252, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 253, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 254, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 255, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 256, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 257, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 258, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 259, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 260, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 261, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 262, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 263, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 264, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 265, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 266, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 267, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 268, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 269, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 270, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 271, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 272, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 273, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 274, loss: 0.05, accuracy: 0.75\n",
      "Epoch 275, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 276, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 277, loss: 0.05, accuracy: 0.75\n",
      "Epoch 278, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 279, loss: 0.05, accuracy: 0.75\n",
      "Epoch 280, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 281, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 282, loss: 0.05, accuracy: 0.75\n",
      "Epoch 283, loss: 0.071875, accuracy: 0.640625\n",
      "Epoch 284, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 285, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 286, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 287, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 288, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 289, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 290, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 291, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 292, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 293, loss: 0.01875, accuracy: 0.90625\n",
      "Epoch 294, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 295, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 296, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 297, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 298, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 299, loss: 0.05, accuracy: 0.75\n",
      "Epoch 300, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 301, loss: 0.05, accuracy: 0.75\n",
      "Epoch 302, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 303, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 304, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 305, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 306, loss: 0.05, accuracy: 0.75\n",
      "Epoch 307, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 308, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 309, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 310, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 311, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 312, loss: 0.05, accuracy: 0.75\n",
      "Epoch 313, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 314, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 315, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 316, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 317, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 318, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 319, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 320, loss: 0.075, accuracy: 0.625\n",
      "Epoch 321, loss: 0.05, accuracy: 0.75\n",
      "Epoch 322, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 323, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 324, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 325, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 326, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 327, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 328, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 329, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 330, loss: 0.05, accuracy: 0.75\n",
      "Epoch 331, loss: 0.05, accuracy: 0.75\n",
      "Epoch 332, loss: 0.025, accuracy: 0.875\n",
      "Epoch 333, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 334, loss: 0.025, accuracy: 0.875\n",
      "Epoch 335, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 336, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 337, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 338, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 339, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 340, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 341, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 342, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 343, loss: 0.05, accuracy: 0.75\n",
      "Epoch 344, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 345, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 346, loss: 0.021875, accuracy: 0.890625\n",
      "Epoch 347, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 348, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 349, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 350, loss: 0.065625, accuracy: 0.671875\n",
      "Epoch 351, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 352, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 353, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 354, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 355, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 356, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 357, loss: 0.028125, accuracy: 0.859375\n",
      "Epoch 358, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 359, loss: 0.05, accuracy: 0.75\n",
      "Epoch 360, loss: 0.05625, accuracy: 0.71875\n",
      "Epoch 361, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 362, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 363, loss: 0.06875, accuracy: 0.65625\n",
      "Epoch 364, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 365, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 366, loss: 0.053125, accuracy: 0.734375\n",
      "Epoch 367, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 368, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 369, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 370, loss: 0.05, accuracy: 0.75\n",
      "Epoch 371, loss: 0.05, accuracy: 0.75\n",
      "Epoch 372, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 373, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 374, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 375, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 376, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 377, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 378, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 379, loss: 0.059375, accuracy: 0.703125\n",
      "Epoch 380, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 381, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 382, loss: 0.025, accuracy: 0.875\n",
      "Epoch 383, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 384, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 385, loss: 0.0375, accuracy: 0.8125\n",
      "Epoch 386, loss: 0.05, accuracy: 0.75\n",
      "Epoch 387, loss: 0.0625, accuracy: 0.6875\n",
      "Epoch 388, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 389, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 390, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 391, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 392, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 393, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 394, loss: 0.046875, accuracy: 0.765625\n",
      "Epoch 395, loss: 0.03125, accuracy: 0.84375\n",
      "Epoch 396, loss: 0.04375, accuracy: 0.78125\n",
      "Epoch 397, loss: 0.040625, accuracy: 0.796875\n",
      "Epoch 398, loss: 0.034375, accuracy: 0.828125\n",
      "Epoch 399, loss: 0.025, accuracy: 0.875\n",
      "loss: 0.04388, accuracy: 0.7806\n"
     ]
    }
   ],
   "source": [
    "#activation function\n",
    "# and regularization\n",
    "\n",
    "nn = NeuralNetwork(test=True, activation_name='sigmoid')\n",
    "nn.TRAIN(X_train, y_train, epochs=400, testing=True)\n",
    "nn.TEST(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef166213a119dea7a0b1a64e774e94a94a893f8530fa4c669d6e23575311d886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
